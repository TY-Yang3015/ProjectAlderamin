optimiser:
  type: "adam"
  adam:
    init_learning_rate: 3e-4
    b1: 0.9
    b2: 0.999

lr:
  delay: 10000.
  decay: 1.

hyperparam:
  batch_size: 4096
  step: 10000
  training_seed: 114514
  gradient_clipping: 0.01
  log_epsilon: 1e-12

  mad_clipping_factor: 5
  scale_input: false

sampler:
  burn_in_steps: 0
  sample_steps: 10
  sampling_seed: 114514
  acceptance_range:
    - 0.5
    - 0.55
  init_width: 1
  sample_width: 0.02
  sample_width_adapt_freq: 100
  computation_dtype: "float32"

psiformer:
  num_of_determinants: 16
  num_of_blocks: 2
  num_heads: 4
  qkv_size: 64
  use_memory_efficient_attention: false
  use_norm: false
  group: null

  computation_dtype: "float32"
  param_dtype: "float32"

ckpt:
  save_ckpt: true
  ckpt_freq: 1000
  save_num_ckpt: 5
