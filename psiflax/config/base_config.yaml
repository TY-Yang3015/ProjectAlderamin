optimiser:
  type: "adam"
  adam:
    init_learning_rate: 3e-4
    b1: 0.9
    b2: 0.999
  shampoo:
    learning_rate: 1e-1
    block_size: 64
    beta1: 0.99
    beta2: 0.999
    diagonal_epsilon: 1e-8
    matrix_epsilon: 1e-8
    weight_decay: 0.0
    start_preconditioning_step: 100
    preconditioning_compute_steps: 1
    statistics_compute_steps: 1
    best_effort_shape_interpretation: true
    nesterov: true
    exponent_override: 0
    shard_optimizer_states: false
    best_effort_memory_usage_reduction: false
    inverse_failure_threshold: 0.1
    moving_average_for_momentum: false
    skip_preconditioning_dim_size_gt: 4096
    decoupled_learning_rate: true
    decoupled_weight_decay: false

lr:
  delay: 10000.
  decay: 2.

hyperparam:
  batch_size: 4096
  step: 10000
  training_seed: 114514
  gradient_clipping: 1
  log_epsilon: 1e-24

  mad_clipping_factor: 5
  scale_input: false

sampler:
  burn_in_steps: 0
  sample_steps: 10
  sampling_seed: 114514
  acceptance_range:
    - 0.5
    - 0.55
  init_width: 1
  sample_width: 0.02
  sample_width_adapt_freq: 100
  computation_dtype: "float32"

psiformer:
  num_of_determinants: 16
  num_of_blocks: 2
  num_heads: 4
  qkv_size: 64
  use_memory_efficient_attention: false
  use_norm: false
  group: null

  computation_dtype: "float32"
  param_dtype: "float32"

ckpt:
  save_ckpt: false
  ckpt_freq: 1000
  save_num_ckpt: 5

log:
  log_grad_and_params: false
  log_pmove: false
